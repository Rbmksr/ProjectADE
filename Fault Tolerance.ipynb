{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b04a48-df44-4dee-9459-51472ad6358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tempfile\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, count, lit\n",
    "import folium  # Import for map visualization\n",
    "import time  # Import for time measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e84091-6bda-46ec-870e-73b806e8a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration variables\n",
    "# Path for local storage of CSV files\n",
    "local_storage_path = \"./data/csv_files\"  # Configurable storage path for CSV files\n",
    "os.makedirs(local_storage_path, exist_ok=True)  # Create the directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ec120a-23d6-487d-8b81-2b3e9ee4a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV (ZIP) URLs\n",
    "csv_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\"]\n",
    "  #  \"https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\",\n",
    "  #  \"https://web.ais.dk/aisdata/aisdk-2024-03-04.zip\",\n",
    "  # \"https://web.ais.dk/aisdata/aisdk-2024-03-05.zip\"\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e5b44-69ee-4580-a0c0-1d3cfc2ee0b9",
   "metadata": {},
   "source": [
    "### How does the system behave under Node/CPU/Memory/Hardware/... errors and failures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c9e4d-2633-4641-8aad-27426426ae2f",
   "metadata": {},
   "source": [
    "### Simulating memory error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45788d5-c5ce-4c22-b9e9-dab67259ccdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OLD VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ea5986-59f2-4228-a982-4253ac1520c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkIllegalArgumentException: [INVALID_EXECUTOR_MEMORY] Executor memory 268435456 must be at least 471859200. Please increase executor memory using the --executor-memory option or \"spark.executor.memory\" in Spark configuration.\r\n\tat org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:230)\r\n\tat org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:201)\r\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:320)\r\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:194)\r\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:478)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# #Schritt 1: Spark-Session erstellen\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# spark = SparkSession.builder \\\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     .appName(\"AIS Data Processing\") \\\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     .getOrCreate()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAIS Data Processing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[2]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m256m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Schritt 2: Funktion zum Herunterladen, Entpacken und Speichern von CSV-Dateien\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_unzip_to_temp_csv\u001b[39m(url):\n",
      "File \u001b[1;32mC:\\users\\prlab\\Documents\\venv-jupyter2\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mC:\\users\\prlab\\Documents\\venv-jupyter2\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mC:\\users\\prlab\\Documents\\venv-jupyter2\\Lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mC:\\users\\prlab\\Documents\\venv-jupyter2\\Lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mC:\\users\\prlab\\Documents\\venv-jupyter2\\Lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\users\\prlab\\Documents\\venv-jupyter2\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\users\\prlab\\Documents\\venv-jupyter2\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkIllegalArgumentException: [INVALID_EXECUTOR_MEMORY] Executor memory 268435456 must be at least 471859200. Please increase executor memory using the --executor-memory option or \"spark.executor.memory\" in Spark configuration.\r\n\tat org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:230)\r\n\tat org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:201)\r\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:320)\r\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:194)\r\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:284)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:478)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# #Schritt 1: Spark-Session erstellen\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"AIS Data Processing\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.executor.memory\", \"256m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schritt 2: Funktion zum Herunterladen, Entpacken und Speichern von CSV-Dateien\n",
    "def download_and_unzip_to_temp_csv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        csv_filename = z.namelist()[0]  # Der Name der CSV-Datei im ZIP-Archiv\n",
    "        with z.open(csv_filename) as csv_file:\n",
    "            temp_file_path = os.path.join(tempfile.gettempdir(), csv_filename)\n",
    "            with open(temp_file_path, \"wb\") as temp_file:\n",
    "                temp_file.write(csv_file.read())\n",
    "            return temp_file_path\n",
    "\n",
    "# Schritt 3: Liste der ZIP-URLs\n",
    "csv_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-04.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-05.zip\"\n",
    "]\n",
    "\n",
    "# Schritt 4: Paralleles Herunterladen und Speichern der CSV-Dateien in temporären Pfaden\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    csv_file_paths = list(executor.map(download_and_unzip_to_temp_csv, csv_urls))\n",
    "\n",
    "# Schritt 5: CSV-Dateien mit Spark einlesen und kombinieren\n",
    "# Erstelle eine Liste von DataFrames für jede CSV-Datei\n",
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in csv_file_paths]\n",
    "\n",
    "# Kombiniere alle DataFrames zu einem großen DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Schritt 6: Einige Beispielzeilen ausgeben, um mögliche MMSI-Nummern anzuzeigen\n",
    "combined_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7155602-7c3e-4d27-b391-51ba843ecd32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NEW VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7b6ca0-3d3c-4547-8522-05d94195e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting: https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\n",
      "Downloading and extracting: https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\n",
      "The download time for 2 files with Spark is 3 minutes and 49 seconds.\n",
      "The README file was created, and the download time has been added.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.executor.memory\", \"256m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Function to download, extract, and save CSV files locally if not already present\n",
    "def download_and_unzip_to_csv(url):\n",
    "    # Extract ZIP filename and corresponding CSV filename\n",
    "    zip_filename = url.split(\"/\")[-1]\n",
    "    csv_filename = zip_filename.replace(\".zip\", \".csv\")\n",
    "    csv_filepath = os.path.join(local_storage_path, csv_filename)\n",
    "    \n",
    "    # Check if the CSV file already exists\n",
    "    if os.path.exists(csv_filepath):\n",
    "        print(f\"File already exists: {csv_filepath}, skipping download.\")\n",
    "        return csv_filepath  # Return the path to the existing file\n",
    "    \n",
    "    # Download and extract the ZIP file\n",
    "    print(f\"Downloading and extracting: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        with z.open(z.namelist()[0]) as csv_file:\n",
    "            # Save the extracted CSV locally\n",
    "            with open(csv_filepath, \"wb\") as output_file:\n",
    "                output_file.write(csv_file.read())\n",
    "    \n",
    "    return csv_filepath  # Return the path to the saved file\n",
    "\n",
    "# Step 4: Parallel downloading and storing CSV files locally\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Count the total number of URLs\n",
    "total_urls = len(csv_urls)\n",
    "\n",
    "# Parallel downloading and storing CSV files\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    csv_file_paths = list(executor.map(download_and_unzip_to_csv, csv_urls))\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Convert elapsed time into minutes and seconds\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "download_time = f\"The download time for {total_urls} files with Spark is {minutes} minutes and {seconds} seconds.\"\n",
    "\n",
    "# Print the download time\n",
    "print(download_time)\n",
    "\n",
    "# Automatically update the README file\n",
    "try:\n",
    "    # Open the file and read its contents\n",
    "    with open(\"README.md\", \"r\") as readme:\n",
    "        lines = readme.readlines()\n",
    "    \n",
    "    # Create a new list of lines\n",
    "    updated_lines = []\n",
    "    section_found = False\n",
    "    for line in lines:\n",
    "        if line.strip() == \"### Download Time Results with Spark\":\n",
    "            # Replace the existing value with the new one\n",
    "            updated_lines.append(line)\n",
    "            updated_lines.append(f\"{download_time}\\n\")\n",
    "            section_found = True\n",
    "        elif not section_found or line.strip() != f\"{download_time}\":\n",
    "            updated_lines.append(line)\n",
    "\n",
    "    # If the section was not found, append it\n",
    "    if not section_found:\n",
    "        updated_lines.append(\"\\n### Download Time Results with Spark\\n\")\n",
    "        updated_lines.append(f\"{download_time}\\n\")\n",
    "\n",
    "    # Overwrite the file\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.writelines(updated_lines)\n",
    "\n",
    "    print(\"The download time and URL count have been successfully updated in the README.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create it\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.write(\"### Download Time Results with Spark\\n\")\n",
    "        readme.write(f\"{download_time}\\n\")\n",
    "    print(\"The README file was created, and the download time has been added.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to the README file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df179154-a864-4152-a130-dfd2cea38591",
   "metadata": {},
   "source": [
    "### What happens during network interruptions and partitioning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74a73e-a965-4f60-bb69-701ba3c11ecd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulating Download fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45377d15-cf0e-4b06-933f-1d9ba2c493a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting: https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Simulated download failure for URL: https://web.ais.dk/aisdata/aisdk-2024-03-01.zip",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Parallel downloading and storing CSV files\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 45\u001b[0m     csv_file_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_and_unzip_to_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_urls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Stop the timer\u001b[39;00m\n\u001b[0;32m     48\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mdownload_and_unzip_to_csv\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m csv_filepath  \u001b[38;5;66;03m# Return the path to the existing file\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:  \u001b[38;5;66;03m# 30% fail\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulated download failure for URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Download and extract the ZIP file\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading and extracting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Simulated download failure for URL: https://web.ais.dk/aisdata/aisdk-2024-03-01.zip"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Function to download, extract, and save CSV files locally if not already present\n",
    "def download_and_unzip_to_csv(url):\n",
    "    # Extract ZIP filename and corresponding CSV filename\n",
    "    zip_filename = url.split(\"/\")[-1]\n",
    "    csv_filename = zip_filename.replace(\".zip\", \".csv\")\n",
    "    csv_filepath = os.path.join(local_storage_path, csv_filename)\n",
    "    \n",
    "    # Check if the CSV file already exists\n",
    "    if os.path.exists(csv_filepath):\n",
    "        print(f\"File already exists: {csv_filepath}, skipping download.\")\n",
    "        return csv_filepath  # Return the path to the existing file\n",
    "        \n",
    "    if random.random() < 0.5:  # 30% fail\n",
    "        raise Exception(f\"Simulated download failure for URL: {url}\")\n",
    "    \n",
    "    # Download and extract the ZIP file\n",
    "    print(f\"Downloading and extracting: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        with z.open(z.namelist()[0]) as csv_file:\n",
    "            # Save the extracted CSV locally\n",
    "            with open(csv_filepath, \"wb\") as output_file:\n",
    "                output_file.write(csv_file.read())\n",
    "    \n",
    "    return csv_filepath  # Return the path to the saved file\n",
    "\n",
    "# Step 4: Parallel downloading and storing CSV files locally\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Count the total number of URLs\n",
    "total_urls = len(csv_urls)\n",
    "\n",
    "# Parallel downloading and storing CSV files\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    csv_file_paths = list(executor.map(download_and_unzip_to_csv, csv_urls))\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Convert elapsed time into minutes and seconds\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "download_time = f\"The download time for {total_urls} files with Spark is {minutes} minutes and {seconds} seconds.\"\n",
    "\n",
    "# Print the download time\n",
    "print(download_time)\n",
    "\n",
    "# Automatically update the README file\n",
    "try:\n",
    "    # Open the file and read its contents\n",
    "    with open(\"README.md\", \"r\") as readme:\n",
    "        lines = readme.readlines()\n",
    "    \n",
    "    # Create a new list of lines\n",
    "    updated_lines = []\n",
    "    section_found = False\n",
    "    for line in lines:\n",
    "        if line.strip() == \"### Download Time Results with Spark\":\n",
    "            # Replace the existing value with the new one\n",
    "            updated_lines.append(line)\n",
    "            updated_lines.append(f\"{download_time}\\n\")\n",
    "            section_found = True\n",
    "        elif not section_found or line.strip() != f\"{download_time}\":\n",
    "            updated_lines.append(line)\n",
    "\n",
    "    # If the section was not found, append it\n",
    "    if not section_found:\n",
    "        updated_lines.append(\"\\n### Download Time Results with Spark\\n\")\n",
    "        updated_lines.append(f\"{download_time}\\n\")\n",
    "\n",
    "    # Overwrite the file\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.writelines(updated_lines)\n",
    "\n",
    "    print(\"The download time and URL count have been successfully updated in the README.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create it\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.write(\"### Download Time Results with Spark\\n\")\n",
    "        readme.write(f\"{download_time}\\n\")\n",
    "    print(\"The README file was created, and the download time has been added.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to the README file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e45de-5e84-48c6-8288-314257907841",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simulating Internet delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004e3f40-302f-4d2f-92cb-ffbc3870bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating network delay: 0.47 seconds for https://web.ais.dk/aisdata/aisdk-2024-03-01.zipSimulating network delay: 0.75 seconds for https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\n",
      "\n",
      "Downloading and extracting: https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\n",
      "Downloading and extracting: https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\n",
      "The download time for 2 files with Spark is 3 minutes and 19 seconds.\n",
      "The download time and URL count have been successfully updated in the README.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Function to download, extract, and save CSV files locally if not already present\n",
    "def download_and_unzip_to_csv(url):\n",
    "    # Extract ZIP filename and corresponding CSV filename\n",
    "    zip_filename = url.split(\"/\")[-1]\n",
    "    csv_filename = zip_filename.replace(\".zip\", \".csv\")\n",
    "    csv_filepath = os.path.join(local_storage_path, csv_filename)\n",
    "    \n",
    "    # Check if the CSV file already exists\n",
    "    if os.path.exists(csv_filepath):\n",
    "        print(f\"File already exists: {csv_filepath}, skipping download.\")\n",
    "        return csv_filepath  # Return the path to the existing file\n",
    "    # delay\n",
    "    delay = random.uniform(0, 5)\n",
    "    print(f\"Simulating network delay: {delay:.2f} seconds for {url}\")\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    # Download and extract the ZIP file\n",
    "    print(f\"Downloading and extracting: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        with z.open(z.namelist()[0]) as csv_file:\n",
    "            # Save the extracted CSV locally\n",
    "            with open(csv_filepath, \"wb\") as output_file:\n",
    "                output_file.write(csv_file.read())\n",
    "    \n",
    "    return csv_filepath  # Return the path to the saved file\n",
    "\n",
    "# Step 4: Parallel downloading and storing CSV files locally\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Count the total number of URLs\n",
    "total_urls = len(csv_urls)\n",
    "\n",
    "# Parallel downloading and storing CSV files\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    csv_file_paths = list(executor.map(download_and_unzip_to_csv, csv_urls))\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Convert elapsed time into minutes and seconds\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "download_time = f\"The download time for {total_urls} files with Spark is {minutes} minutes and {seconds} seconds.\"\n",
    "\n",
    "# Print the download time\n",
    "print(download_time)\n",
    "\n",
    "# Automatically update the README file\n",
    "try:\n",
    "    # Open the file and read its contents\n",
    "    with open(\"README.md\", \"r\") as readme:\n",
    "        lines = readme.readlines()\n",
    "    \n",
    "    # Create a new list of lines\n",
    "    updated_lines = []\n",
    "    section_found = False\n",
    "    for line in lines:\n",
    "        if line.strip() == \"### Download Time Results with Spark\":\n",
    "            # Replace the existing value with the new one\n",
    "            updated_lines.append(line)\n",
    "            updated_lines.append(f\"{download_time}\\n\")\n",
    "            section_found = True\n",
    "        elif not section_found or line.strip() != f\"{download_time}\":\n",
    "            updated_lines.append(line)\n",
    "\n",
    "    # If the section was not found, append it\n",
    "    if not section_found:\n",
    "        updated_lines.append(\"\\n### Download Time Results with Spark\\n\")\n",
    "        updated_lines.append(f\"{download_time}\\n\")\n",
    "\n",
    "    # Overwrite the file\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.writelines(updated_lines)\n",
    "\n",
    "    print(\"The download time and URL count have been successfully updated in the README.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create it\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.write(\"### Download Time Results with Spark\\n\")\n",
    "        readme.write(f\"{download_time}\\n\")\n",
    "    print(\"The README file was created, and the download time has been added.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to the README file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56f034-c860-4db4-8161-b37f100df450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  How do error handling mechanisms affect efficiency/scale/latency/throughput/... etc.? Are there any worst/best case considerations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d665629e-e172-4fde-b8bc-cbb152df5cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating network delay: 1.38 seconds for https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\n",
      "Simulating network delay: 3.42 seconds for https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\n",
      "Simulating network delay: 0.76 seconds for https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\n",
      "Downloading and extracting: https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\n",
      "Downloading and extracting: https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\n",
      "Unhandled error for URL https://web.ais.dk/aisdata/aisdk-2024-03-02.zip: Simulated download failure for URL: https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\n",
      "\n",
      "Download Summary:\n",
      "Total URLs: 3\n",
      "Successful Downloads: 2\n",
      "Failed Downloads: 1\n",
      "Total time: 3 minutes and 25 seconds.\n",
      "The README file was updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Local storage path\n",
    "local_storage_path = \"./data\"\n",
    "os.makedirs(local_storage_path, exist_ok=True)\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing with Fault Tolerance\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example CSV URLs\n",
    "csv_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\"\n",
    "]\n",
    "\n",
    "# Simulate download and extraction with failure and delay\n",
    "def download_and_unzip_to_csv(url):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file, extracts its contents, and saves as a CSV.\n",
    "    Introduces random failures and delays to simulate errors.\n",
    "    \"\"\"\n",
    "    zip_filename = url.split(\"/\")[-1]\n",
    "    csv_filename = zip_filename.replace(\".zip\", \".csv\")\n",
    "    csv_filepath = os.path.join(local_storage_path, csv_filename)\n",
    "\n",
    "    # Check if the CSV already exists\n",
    "    if os.path.exists(csv_filepath):\n",
    "        print(f\"File already exists: {csv_filepath}, skipping download.\")\n",
    "        return csv_filepath  # Return existing file path\n",
    "\n",
    "    # Simulate network delay\n",
    "    delay = random.uniform(0, 5)  # Random delay between 0-5 seconds\n",
    "    print(f\"Simulating network delay: {delay:.2f} seconds for {url}\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "    # Simulate random failure\n",
    "    if random.random() < 0.3:  # 30% chance of failure\n",
    "        raise Exception(f\"Simulated download failure for URL: {url}\")\n",
    "\n",
    "    try:\n",
    "        # Download and extract the ZIP file\n",
    "        print(f\"Downloading and extracting: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        zipfile_bytes = BytesIO(response.content)\n",
    "        with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "            with z.open(z.namelist()[0]) as csv_file:\n",
    "                with open(csv_filepath, \"wb\") as output_file:\n",
    "                    output_file.write(csv_file.read())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return None  # Return None for failed downloads\n",
    "\n",
    "    return csv_filepath  # Return path to saved file\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Parallel downloading with fault tolerance\n",
    "total_urls = len(csv_urls)\n",
    "successful_downloads = 0\n",
    "failed_downloads = 0\n",
    "download_times = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_to_url = {executor.submit(download_and_unzip_to_csv, url): url for url in csv_urls}\n",
    "    for future in as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                successful_downloads += 1\n",
    "            else:\n",
    "                failed_downloads += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Unhandled error for URL {url}: {e}\")\n",
    "            failed_downloads += 1\n",
    "\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "total_time = f\"Total time: {minutes} minutes and {seconds} seconds.\"\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nDownload Summary:\")\n",
    "print(f\"Total URLs: {total_urls}\")\n",
    "print(f\"Successful Downloads: {successful_downloads}\")\n",
    "print(f\"Failed Downloads: {failed_downloads}\")\n",
    "print(total_time)\n",
    "\n",
    "# Automatically update the README file\n",
    "try:\n",
    "    with open(\"README.md\", \"a\") as readme:\n",
    "        readme.write(\"\\n### Download Time and Results\\n\")\n",
    "        readme.write(f\"Total URLs: {total_urls}\\n\")\n",
    "        readme.write(f\"Successful Downloads: {successful_downloads}\\n\")\n",
    "        readme.write(f\"Failed Downloads: {failed_downloads}\\n\")\n",
    "        readme.write(f\"{total_time}\\n\")\n",
    "    print(\"The README file was updated successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to the README file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185acf0",
   "metadata": {},
   "source": [
    "#### Fault Tolerance on Cluster - Spark Standalone Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa71665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up spark in cluster mode with two executors\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://127.0.0.1:7077\") \\\n",
    "    .config(\"spark.executor.instances\", 2) \\\n",
    "    .config(\"spark.executor.cores\",\"2\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
