{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a69d9-359d-4e41-9797-9fde49108416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark-Session starten\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ShipRouteOptimization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Optional: Log-Level reduzieren für weniger Output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cacfe2-452c-4dcb-a8ab-f1461d2352ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           # Timestamp Type of mobile       MMSI   Latitude  Longitude  \\\n",
      "0  01/03/2024 00:00:00        Class A  219000873  56.990910  10.304543   \n",
      "1  01/03/2024 00:00:00   Base Station    2190068  56.447260  10.945872   \n",
      "2  01/03/2024 00:00:00        Class A  219016683  56.800165   9.024933   \n",
      "3  01/03/2024 00:00:00        Class A  219000615  56.967093   9.224287   \n",
      "4  01/03/2024 00:00:00   Base Station    2190071  57.110043   8.648282   \n",
      "\n",
      "          Navigational status  ROT  SOG    COG  Heading  ... Length  \\\n",
      "0      Under way using engine  NaN  0.0   30.2      NaN  ...    NaN   \n",
      "1               Unknown value  NaN  NaN    NaN      NaN  ...    NaN   \n",
      "2      Under way using engine  0.0  0.0  257.3     17.0  ...    NaN   \n",
      "3  Restricted maneuverability  0.0  2.3   69.8     67.0  ...    NaN   \n",
      "4               Unknown value  NaN  NaN    NaN      NaN  ...    NaN   \n",
      "\n",
      "  Type of position fixing device Draught Destination  ETA  Data source type  \\\n",
      "0                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "1                            GPS     NaN     Unknown  NaN               AIS   \n",
      "2                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "3                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "4                            GPS     NaN     Unknown  NaN               AIS   \n",
      "\n",
      "    A   B   C   D  \n",
      "0 NaN NaN NaN NaN  \n",
      "1 NaN NaN NaN NaN  \n",
      "2 NaN NaN NaN NaN  \n",
      "3 NaN NaN NaN NaN  \n",
      "4 NaN NaN NaN NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import folium\n",
    "\n",
    "# Schritt 1: Funktion zum Herunterladen und Entpacken von ZIP-Archiv und Laden der CSV\n",
    "def download_and_unzip_csv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        csv_filename = z.namelist()[0] \n",
    "        with z.open(csv_filename) as csv_file:\n",
    "            return pd.read_csv(csv_file)\n",
    "\n",
    "# Schritt 2: Liste der CSV (ZIP)-URLs\n",
    "csv_urls = [\"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\", \n",
    "            \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\"\n",
    "           ]\n",
    "\n",
    "# Schritt 3: Paralleles Herunterladen und Verarbeiten der ZIP-Dateien\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    dfs = list(executor.map(download_and_unzip_csv, csv_urls))\n",
    "\n",
    "# Schritt 4: Alle DataFrames in einen großen DataFrame kombinieren\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Einige Beispielzeilen ausgeben um Mögliche MMSI-Nummern anzuzeigen\n",
    "print(combined_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34ace257-2e80-48d9-8233-e08eeecddef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnung: 'Type of mobile' Spalte nicht vorhanden, Überspringe diesen Schritt.\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# 2. Basisstationen herausfiltern (\"Type of mobile\" != \"Base Station\")\n",
    "########################################################\n",
    "\n",
    "# Nur Zeilen behalten, die keine \"Base Station\" sind\n",
    "if \"Type of mobile\" in combined_df.columns:\n",
    "    combined_df = combined_df[combined_df[\"Type of mobile\"] != \"Base Station\"]\n",
    "else:\n",
    "    print(\"Warnung: 'Type of mobile' Spalte nicht vorhanden, Überspringe diesen Schritt.\")\n",
    "\n",
    "########################################################\n",
    "# 3. Nur relevante Spalten behalten, um Datenmenge zu reduzieren\n",
    "########################################################\n",
    "\n",
    "relevant_columns = [\"MMSI\", \"Latitude\", \"Longitude\", \"# Timestamp\"]\n",
    "combined_df = combined_df[relevant_columns]\n",
    "\n",
    "########################################################\n",
    "# 4. Timestamp in datetime konvertieren\n",
    "########################################################\n",
    "\n",
    "combined_df['# Timestamp'] = pd.to_datetime(combined_df['# Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "########################################################\n",
    "# 5. Filtern von MMSI-Nummern, die genug Datenpunkte haben\n",
    "########################################################\n",
    "\n",
    "# Anzahl Datenpunkte pro MMSI bestimmen\n",
    "mmsi_counts = combined_df.groupby(\"MMSI\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Schwelle definieren (z.B. mindestens 50 Punkte)\n",
    "threshold = 50\n",
    "valid_mmsi = mmsi_counts[mmsi_counts[\"count\"] >= threshold][\"MMSI\"].unique()\n",
    "\n",
    "# Gefilterter DataFrame nur mit MMSI, die genügend Datenpunkte haben\n",
    "filtered_by_count_df = combined_df[combined_df[\"MMSI\"].isin(valid_mmsi)]\n",
    "\n",
    "########################################################\n",
    "# 6. Nach bestimmter MMSI und Datum filtern + Route plotten\n",
    "########################################################\n",
    "\n",
    "mmsi_number = 211417590  # Ersetze mit deiner MMSI\n",
    "desired_date = \"2024-03-01\" # Beispiel-Datum\n",
    "\n",
    "# Prüfen, ob MMSI genug Daten hat\n",
    "if mmsi_number not in valid_mmsi:\n",
    "    print(f\"MMSI {mmsi_number} hat nicht genügend Datenpunkte, um eine aussagekräftige Route anzuzeigen.\")\n",
    "else:\n",
    "    # Nach MMSI und Datum filtern\n",
    "    route_df = filtered_by_count_df[\n",
    "        (filtered_by_count_df[\"MMSI\"] == mmsi_number) &\n",
    "        (filtered_by_count_df[\"# Timestamp\"].dt.date == pd.to_datetime(desired_date).date())\n",
    "    ].sort_values(\"# Timestamp\")\n",
    "\n",
    "    # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "    if route_df.empty:\n",
    "        print(f\"Keine Daten für MMSI {mmsi_number} am {desired_date}\")\n",
    "    else:\n",
    "        # Karte erstellen und Route plotten\n",
    "        mean_lat = route_df[\"Latitude\"].mean()\n",
    "        mean_lon = route_df[\"Longitude\"].mean()\n",
    "        \n",
    "        route_map = folium.Map(location=[mean_lat, mean_lon], zoom_start=8)\n",
    "        \n",
    "        # Koordinatenliste für PolyLine\n",
    "        coords = route_df[[\"Latitude\", \"Longitude\"]].values.tolist()\n",
    "        \n",
    "        # PolyLine hinzufügen\n",
    "        folium.PolyLine(coords, color=\"blue\", weight=2.5, opacity=1).add_to(route_map)\n",
    "        \n",
    "        # Optional: Punkte markieren (auskommentiert lassen, falls nicht benötigt)\n",
    "        # for _, row in route_df.iterrows():\n",
    "        #     folium.CircleMarker(\n",
    "        #         location=[row['Latitude'], row['Longitude']],\n",
    "        #         radius=2,\n",
    "        #         color='red',\n",
    "        #         fill=True,\n",
    "        #         fill_color='red',\n",
    "        #         fill_opacity=0.7,\n",
    "        #         popup=f\"Timestamp: {row['# Timestamp']}\"\n",
    "        #     ).add_to(route_map)\n",
    "        \n",
    "        # Karte anzeigen oder speichern\n",
    "        route_map.save(\"ship_route.html\")\n",
    "        route_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f66f2c26-fb3b-4515-a6d1-5ff42e2115d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MMSI', 'Latitude', 'Longitude', '# Timestamp'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/15 11:47:20 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/21/3rh2twzd4z7fclyj9p6q77hh0000gn/T/blockmgr-43195ef6-0b66-4d07-8032-5ec23e0d3855. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/21/3rh2twzd4z7fclyj9p6q77hh0000gn/T/blockmgr-43195ef6-0b66-4d07-8032-5ec23e0d3855\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:166)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.IOException: Cannot run program \"rm\": error=0, Failed to exec spawn helper: pid: 21574, signal: 15\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:162)\n",
      "\t... 33 more\n",
      "Caused by: java.io.IOException: error=0, Failed to exec spawn helper: pid: 21574, signal: 15\n",
      "\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n",
      "\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n",
      "\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n",
      "\t... 35 more\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e6741-819b-4513-8482-8e17696b6411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
