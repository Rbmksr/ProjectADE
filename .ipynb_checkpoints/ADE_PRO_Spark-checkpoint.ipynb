{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abc1419-5f49-4b88-a7b1-e898431ee10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Data Processing with Spark vs. Pandas\n",
    "# -------------------------------------------------\n",
    "#\n",
    "# Goal of this Notebook:\n",
    "# - Compare the data processing speed between Apache Spark and Pandas.\n",
    "# - Analyze the data size limits where Pandas reaches its boundaries and Spark shows its advantages.\n",
    "#\n",
    "# Overview:\n",
    "# - The first code block implements data processing with Spark.\n",
    "# - The second code block implements the same logic using Pandas.\n",
    "# - Both approaches are compared in terms of runtime and memory usage.\n",
    "#\n",
    "# Prerequisites:\n",
    "# - Python version: 3.8 or higher\n",
    "# - Apache Spark: 3.5.0\n",
    "# - Installed libraries: pyspark, pandas, requests, folium\n",
    "#\n",
    "# Workflow:\n",
    "# 1. Download and prepare the data.\n",
    "# 2. Process the data using both approaches.\n",
    "# 3. Measure and compare the runtime.\n",
    "#\n",
    "# Let's start by importing the necessary libraries and setting up the Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8f7229-a936-4410-be9c-065ec2a8f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tempfile\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, count, lit\n",
    "import folium  # Import for map visualization\n",
    "import time  # Import for time measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2632a8-ae20-4a15-9e66-a0093c188de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration variables\n",
    "# Path for temporary storage\n",
    "temp_storage_path = \"./data/temp\"  # Configurable storage path for temporary files\n",
    "os.makedirs(temp_storage_path, exist_ok=True)  # Create the directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60415093-8a2a-443b-a60c-efa73c858684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV (ZIP) URLs\n",
    "csv_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\"]\n",
    "  #  \"https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\",\n",
    "  #  \"https://web.ais.dk/aisdata/aisdk-2024-03-04.zip\",\n",
    "  # \"https://web.ais.dk/aisdata/aisdk-2024-03-05.zip\"\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc77e413-36d9-4d66-83fb-98656c3e9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Download-Zeit f체r 2 Dateien mit Spark betr채gt 6 Minuten und 46 Sekunden.\n",
      "Die Download-Zeit und URL-Anzahl wurden erfolgreich aktualisiert.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Function to download, extract, and save CSV files\n",
    "def download_and_unzip_to_temp_csv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        csv_filename = z.namelist()[0]  # Name of the CSV file in the ZIP archive\n",
    "        with z.open(csv_filename) as csv_file:\n",
    "            temp_file_path = os.path.join(tempfile.gettempdir(), csv_filename)\n",
    "            with open(temp_file_path, \"wb\") as temp_file:\n",
    "                temp_file.write(csv_file.read())\n",
    "            return temp_file_path\n",
    "\n",
    "# Step 4: Parallel downloading and storing CSV files in temporary paths\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Count the total number of URLs\n",
    "total_urls = len(csv_urls)\n",
    "\n",
    "# Parallel downloading and storing CSV files\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    csv_file_paths = list(executor.map(download_and_unzip_to_temp_csv, csv_urls))\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Convert elapsed time into minutes and seconds\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "download_time = f\"The download time for {total_urls} files with Spark is {minutes} minutes and {seconds} seconds.\"\n",
    "\n",
    "# Print the download time\n",
    "print(download_time)\n",
    "\n",
    "# Automatically update the README file\n",
    "try:\n",
    "    # Open the file and read its contents\n",
    "    with open(\"README.md\", \"r\") as readme:\n",
    "        lines = readme.readlines()\n",
    "    \n",
    "    # Create a new list of lines\n",
    "    updated_lines = []\n",
    "    section_found = False\n",
    "    for line in lines:\n",
    "        if line.strip() == \"### Download Time Results with Spark\":\n",
    "            # Replace the existing value with the new one\n",
    "            updated_lines.append(line)\n",
    "            updated_lines.append(f\"{download_time}\\n\")\n",
    "            section_found = True\n",
    "        elif not section_found or line.strip() != f\"{download_time}\":\n",
    "            updated_lines.append(line)\n",
    "\n",
    "    # If the section was not found, append it\n",
    "    if not section_found:\n",
    "        updated_lines.append(\"\\n### Download Time Results with Spark\\n\")\n",
    "        updated_lines.append(f\"{download_time}\\n\")\n",
    "\n",
    "    # Overwrite the file\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.writelines(updated_lines)\n",
    "\n",
    "    print(\"The download time and URL count have been successfully updated in the README.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create it\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.write(\"### Download Time Results with Spark\\n\")\n",
    "        readme.write(f\"{download_time}\\n\")\n",
    "    print(\"The README file was created, and the download time has been added.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to the README file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68a5d3-8820-44d1-9240-23f1ed084288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Read CSV files with Spark and combine them\n",
    "# Create a list of DataFrames for each CSV file\n",
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in csv_file_paths]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca1ee5e0-3317-43d3-87e8-c559bfa8a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "|        # Timestamp|Type of mobile|     MMSI| Latitude|Longitude| Navigational status| ROT| SOG|  COG|Heading|    IMO|Callsign|Name|Ship type|Cargo type|Width|Length|Type of position fixing device|Draught|Destination| ETA|Data source type|   A|   B|   C|   D|\n",
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "|01/03/2024 00:00:00|       Class A|219000873| 56.99091|10.304543|Under way using e...|NULL| 0.0| 30.2|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|  Base Station|  2190068| 56.44726|10.945872|       Unknown value|NULL|NULL| NULL|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                           GPS|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219016683|56.800165| 9.024933|Under way using e...| 0.0| 0.0|257.3|     17|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219000615|56.967093| 9.224287|Restricted maneuv...| 0.0| 2.3| 69.8|     67|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|  Base Station|  2190071|57.110043| 8.648282|       Unknown value|NULL|NULL| NULL|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                           GPS|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219017664| 56.97495|  8.92253|Under way using e...| 0.0| 0.0|349.9|    201|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219002686|56.795143|  8.86396|Under way using e...| 0.0| 0.0|116.1|    213|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219030053|57.058252| 9.900817|       Unknown value|NULL| 0.0| 15.8|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219670000|55.463782| 8.444915|              Moored| 0.0| 0.0|321.0|    118|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|211417590|54.524345|12.675237|Under way using e...| 0.0| 0.6|286.8|     23|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:====================================================>   (42 + 3) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Originale Datensatz hat 31817670 Eintr채ge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 6: Display some sample rows to show possible MMSI numbers\n",
    "combined_df.show(10)\n",
    "\n",
    "# Print the original number of entries\n",
    "print(f\"The original dataset contains {combined_df.count()} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80f285-861f-468b-b1db-f64cee9a2371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f31b2-e8de-4809-8142-eeade1487d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Read CSV files with Spark and combine them\n",
    "# Create a list of DataFrames for each CSV file\n",
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in csv_file_paths]\n",
    "\n",
    "# Combine all DataFrames into one large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec710fc9-c2e8-441c-ae18-4fca9a94efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:====================================================>   (42 + 3) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der angepasste Datensatz hat 29508390 Eintr채ge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# 2. Filter out base stations as they do not display navigation data (\"Type of mobile\" != \"Base Station\")\n",
    "########################################################\n",
    "\n",
    "# Check if the column \"Type of mobile\" exists and filter\n",
    "if \"Type of mobile\" in combined_df.columns:\n",
    "    combined_df = combined_df.filter(col(\"Type of mobile\") != \"Base Station\")\n",
    "else:\n",
    "    print(\"Warning: 'Type of mobile' column not found, skipping this step.\")\n",
    "\n",
    "print(f\"The adjusted dataset contains {combined_df.count()} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a4d614e-9200-4332-ad5f-fb8feacd2d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# 3. Keep only relevant columns to reduce data size\n",
    "########################################################\n",
    "\n",
    "relevant_columns = [\"MMSI\", \"Latitude\", \"Longitude\", \"# Timestamp\"]\n",
    "combined_df = combined_df.select(*relevant_columns)\n",
    "\n",
    "########################################################\n",
    "# 4. Convert Timestamp to datetime format\n",
    "########################################################\n",
    "\n",
    "combined_df = combined_df.withColumn(\"# Timestamp\", to_timestamp(col(\"# Timestamp\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "########################################################\n",
    "# 5. Filter MMSI numbers with enough data points to display meaningful routes\n",
    "########################################################\n",
    "\n",
    "# Count the number of data points per MMSI\n",
    "mmsi_counts = combined_df.groupBy(\"MMSI\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Define a threshold (e.g., at least 50 points)\n",
    "threshold = 50\n",
    "valid_mmsi = mmsi_counts.filter(col(\"count\") >= threshold).select(\"MMSI\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filtered DataFrame containing only MMSI numbers with sufficient data points\n",
    "filtered_by_count_df = combined_df.filter(col(\"MMSI\").isin(valid_mmsi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "065e73f9-4d8e-47ee-a11d-c908afb1d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route wurde erfolgreich als 'ship_route.html' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# 6. Filter by specific MMSI and time range + plot the route\n",
    "########################################################\n",
    "\n",
    "mmsi_number = 219016832  # Replace with your desired MMSI\n",
    "\n",
    "# Define start and end timestamps (in the format \"dd/MM/yyyy HH:mm:ss\")\n",
    "start_str = \"01/03/2024 00:00:00\"  # Start time\n",
    "end_str = \"01/03/2024 06:59:59\"    # End time\n",
    "\n",
    "# Convert start and end times to datetime objects\n",
    "start_dt = to_timestamp(lit(start_str), \"dd/MM/yyyy HH:mm:ss\")\n",
    "end_dt = to_timestamp(lit(end_str), \"dd/MM/yyyy HH:mm:ss\")\n",
    "\n",
    "# Check if the MMSI has enough data points\n",
    "if mmsi_number not in valid_mmsi:\n",
    "    print(f\"MMSI {mmsi_number} does not have enough data points to display a meaningful route.\")\n",
    "else:\n",
    "    # Filter by MMSI and time range\n",
    "    route_df = filtered_by_count_df.filter(\n",
    "        (col(\"MMSI\") == mmsi_number) &\n",
    "        (col(\"# Timestamp\") >= start_dt) &\n",
    "        (col(\"# Timestamp\") <= end_dt)\n",
    "    ).orderBy(\"# Timestamp\")\n",
    "\n",
    "    # Check if filtered data is available\n",
    "    if route_df.count() == 0:\n",
    "        print(f\"No data for MMSI {mmsi_number} between {start_str} and {end_str}\")\n",
    "    else:\n",
    "        # Convert data to Pandas DataFrame for plotting\n",
    "        pandas_df = route_df.toPandas()\n",
    "\n",
    "        # Create a map and plot the route\n",
    "        mean_lat = pandas_df[\"Latitude\"].mean()\n",
    "        mean_lon = pandas_df[\"Longitude\"].mean()\n",
    "        \n",
    "        route_map = folium.Map(location=[mean_lat, mean_lon], zoom_start=8)\n",
    "        \n",
    "        # Create a list of coordinates for the PolyLine\n",
    "        coords = pandas_df[[\"Latitude\", \"Longitude\"]].values.tolist()\n",
    "        \n",
    "        # Add the PolyLine to the map\n",
    "        folium.PolyLine(coords, color=\"blue\", weight=2.5, opacity=1).add_to(route_map)\n",
    "        \n",
    "        # Save the map\n",
    "        route_map.save(\"ship_route.html\")\n",
    "        print(\"Route successfully saved as 'ship_route.html'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
