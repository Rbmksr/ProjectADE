{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ea5986-59f2-4228-a982-4253ac1520c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "|        # Timestamp|Type of mobile|     MMSI| Latitude|Longitude| Navigational status| ROT| SOG|  COG|Heading|    IMO|Callsign|Name|Ship type|Cargo type|Width|Length|Type of position fixing device|Draught|Destination| ETA|Data source type|   A|   B|   C|   D|\n",
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "|01/03/2024 00:00:00|       Class A|219000873| 56.99091|10.304543|Under way using e...|NULL| 0.0| 30.2|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|  Base Station|  2190068| 56.44726|10.945872|       Unknown value|NULL|NULL| NULL|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                           GPS|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219016683|56.800165| 9.024933|Under way using e...| 0.0| 0.0|257.3|     17|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219000615|56.967093| 9.224287|Restricted maneuv...| 0.0| 2.3| 69.8|     67|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|  Base Station|  2190071|57.110043| 8.648282|       Unknown value|NULL|NULL| NULL|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                           GPS|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219017664| 56.97495|  8.92253|Under way using e...| 0.0| 0.0|349.9|    201|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219002686|56.795143|  8.86396|Under way using e...| 0.0| 0.0|116.1|    213|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219030053|57.058252| 9.900817|       Unknown value|NULL| 0.0| 15.8|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219670000|55.463782| 8.444915|              Moored| 0.0| 0.0|321.0|    118|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|211417590|54.524345|12.675237|Under way using e...| 0.0| 0.6|286.8|     23|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Schritt 1: Spark-Session erstellen\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schritt 2: Funktion zum Herunterladen, Entpacken und Speichern von CSV-Dateien\n",
    "def download_and_unzip_to_temp_csv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        csv_filename = z.namelist()[0]  # Der Name der CSV-Datei im ZIP-Archiv\n",
    "        with z.open(csv_filename) as csv_file:\n",
    "            temp_file_path = os.path.join(tempfile.gettempdir(), csv_filename)\n",
    "            with open(temp_file_path, \"wb\") as temp_file:\n",
    "                temp_file.write(csv_file.read())\n",
    "            return temp_file_path\n",
    "\n",
    "# Schritt 3: Liste der ZIP-URLs\n",
    "csv_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-04.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-05.zip\"\n",
    "]\n",
    "\n",
    "# Schritt 4: Paralleles Herunterladen und Speichern der CSV-Dateien in temporären Pfaden\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    csv_file_paths = list(executor.map(download_and_unzip_to_temp_csv, csv_urls))\n",
    "\n",
    "# Schritt 5: CSV-Dateien mit Spark einlesen und kombinieren\n",
    "# Erstelle eine Liste von DataFrames für jede CSV-Datei\n",
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in csv_file_paths]\n",
    "\n",
    "# Kombiniere alle DataFrames zu einem großen DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Schritt 6: Einige Beispielzeilen ausgeben, um mögliche MMSI-Nummern anzuzeigen\n",
    "combined_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5202c-81d4-4877-b64d-0e1dbd3aecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Versuch: Download und entpacken der drei ZIP-Files dauert 08:41,45 min\n",
    "# 2.Versuch: Download und entpacken der fünf ZIP-Files dauert 10:20,32min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9571b0f-0ae1-4357-9164-754dba364c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Originale Datensatz hat 72413518 Einträge.\n",
      "Warnung: 'Type of mobile' Spalte nicht vorhanden, Überspringe diesen Schritt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der angepasste Datensatz hat 72413518 Einträge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, count\n",
    "\n",
    "# Originale Anzahl der Einträge ausgeben\n",
    "print(f\"Der Originale Datensatz hat {combined_df.count()} Einträge.\")\n",
    "\n",
    "########################################################\n",
    "# 2. Basisstationen herausfiltern (\"Type of mobile\" != \"Base Station\")\n",
    "########################################################\n",
    "\n",
    "# Prüfen, ob die Spalte \"Type of mobile\" existiert und filtern\n",
    "if \"Type of mobile\" in combined_df.columns:\n",
    "    combined_df = combined_df.filter(col(\"Type of mobile\") != \"Base Station\")\n",
    "else:\n",
    "    print(\"Warnung: 'Type of mobile' Spalte nicht vorhanden, Überspringe diesen Schritt.\")\n",
    "\n",
    "print(f\"Der angepasste Datensatz hat {combined_df.count()} Einträge.\")\n",
    "\n",
    "########################################################\n",
    "# 3. Nur relevante Spalten behalten, um Datenmenge zu reduzieren\n",
    "########################################################\n",
    "\n",
    "relevant_columns = [\"MMSI\", \"Latitude\", \"Longitude\", \"# Timestamp\"]\n",
    "combined_df = combined_df.select(*relevant_columns)\n",
    "\n",
    "########################################################\n",
    "# 4. Timestamp in datetime konvertieren\n",
    "########################################################\n",
    "\n",
    "combined_df = combined_df.withColumn(\"# Timestamp\", to_timestamp(col(\"# Timestamp\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "########################################################\n",
    "# 5. Filtern von MMSI-Nummern, die genug Datenpunkte haben\n",
    "########################################################\n",
    "\n",
    "# Anzahl Datenpunkte pro MMSI bestimmen\n",
    "mmsi_counts = combined_df.groupBy(\"MMSI\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Schwelle definieren (z.B. mindestens 50 Punkte)\n",
    "threshold = 50\n",
    "valid_mmsi = mmsi_counts.filter(col(\"count\") >= threshold).select(\"MMSI\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Gefilterter DataFrame nur mit MMSI, die genügend Datenpunkte haben\n",
    "filtered_by_count_df = combined_df.filter(col(\"MMSI\").isin(valid_mmsi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5e87d9-1a40-422c-bfcf-659fca43bc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import folium  # Import für Kartenvisualisierung hinzugefügt\n",
    "from pyspark.sql.functions import lit\n",
    "########################################################\n",
    "# 6. Nach bestimmter MMSI und Zeitspanne filtern + Route plotten\n",
    "########################################################\n",
    "\n",
    "mmsi_number = 219016832  # Ersetze mit deiner MMSI\n",
    "\n",
    "# Definiere Start- und Endzeitpunkt (im Format \"dd/MM/yyyy HH:MM:SS\")\n",
    "start_str = \"03/03/2024 00:00:00\"  # 1. März 2024, 00:00 Uhr\n",
    "end_str = \"03/03/2024 06:59:59\"    # 3. März 2024, 23:59:59\n",
    "\n",
    "# Konvertiere Start- und Endzeit in datetime-Objekte\n",
    "start_dt = to_timestamp(lit(start_str), \"dd/MM/yyyy HH:mm:ss\")\n",
    "end_dt = to_timestamp(lit(end_str), \"dd/MM/yyyy HH:mm:ss\")\n",
    "\n",
    "# Prüfen, ob MMSI genug Daten hat\n",
    "if mmsi_number not in valid_mmsi:\n",
    "    print(f\"MMSI {mmsi_number} hat nicht genügend Datenpunkte, um eine aussagekräftige Route anzuzeigen.\")\n",
    "else:\n",
    "    # Nach MMSI und Zeitspanne filtern\n",
    "    route_df = filtered_by_count_df.filter(\n",
    "        (col(\"MMSI\") == mmsi_number) &\n",
    "        (col(\"# Timestamp\") >= start_dt) &\n",
    "        (col(\"# Timestamp\") <= end_dt)\n",
    "    ).orderBy(\"# Timestamp\")\n",
    "\n",
    "    # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "    if route_df.count() == 0:\n",
    "        print(f\"Keine Daten für MMSI {mmsi_number} zwischen {start_str} und {end_str}\")\n",
    "    else:\n",
    "        # Daten in Pandas konvertieren, um Karte zu erstellen\n",
    "        pandas_df = route_df.toPandas()\n",
    "\n",
    "        # Karte erstellen und Route plotten\n",
    "        mean_lat = pandas_df[\"Latitude\"].mean()\n",
    "        mean_lon = pandas_df[\"Longitude\"].mean()\n",
    "        \n",
    "        route_map = folium.Map(location=[mean_lat, mean_lon], zoom_start=8)\n",
    "        \n",
    "        # Koordinatenliste für PolyLine\n",
    "        coords = pandas_df[[\"Latitude\", \"Longitude\"]].values.tolist()\n",
    "        \n",
    "        # PolyLine hinzufügen\n",
    "        folium.PolyLine(coords, color=\"blue\", weight=2.5, opacity=1).add_to(route_map)\n",
    "        \n",
    "        # Optional: Punkte markieren (auskommentiert lassen, falls nicht benötigt)\n",
    "        # for _, row in pandas_df.iterrows():\n",
    "        #     folium.CircleMarker(\n",
    "        #         location=[row['Latitude'], row['Longitude']],\n",
    "        #         radius=2,\n",
    "        #         color='red',\n",
    "        #         fill=True,\n",
    "        #         fill_color='red',\n",
    "        #         fill_opacity=0.7,\n",
    "        #         popup=f\"Timestamp: {row['# Timestamp']}\"\n",
    "        #     ).add_to(route_map)\n",
    "        \n",
    "        # Karte anzeigen oder speichern\n",
    "        route_map.save(\"ship_route.html\")\n",
    "        route_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77f899-8fb1-4151-82fa-b5d4fc302fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
