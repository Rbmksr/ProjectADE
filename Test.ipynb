{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc1419-5f49-4b88-a7b1-e898431ee10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich: Datenverarbeitung mit Spark vs. Pandas\n",
    "# -------------------------------------------------\n",
    "#\n",
    "# Ziel dieses Notebooks:\n",
    "# - Vergleich der Datenverarbeitungsgeschwindigkeit zwischen Apache Spark und Pandas.\n",
    "# - Analyse, bei welchen Datenmengen Pandas an seine Grenzen stößt und Spark seine Vorteile zeigt.\n",
    "#\n",
    "# Überblick:\n",
    "# - Der erste Codeblock implementiert die Datenverarbeitung mit Spark.\n",
    "# - Der zweite Codeblock implementiert die gleiche Logik mit Pandas.\n",
    "# - Beide Ansätze werden in Bezug auf Laufzeit und Speicherbedarf verglichen.\n",
    "#\n",
    "# Voraussetzungen:\n",
    "# - Python-Version: 3.8 oder höher\n",
    "# - Apache Spark: 3.5.0\n",
    "# - Installierte Bibliotheken: pyspark, pandas, requests, folium\n",
    "#\n",
    "# Ablauf:\n",
    "# 1. Daten herunterladen und vorbereiten.\n",
    "# 2. Daten mit beiden Ansätzen verarbeiten.\n",
    "# 3. Laufzeit messen und vergleichen.\n",
    "#\n",
    "# Starten wir mit dem Import der notwendigen Bibliotheken und dem Setup der Spark-Umgebung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8f7229-a936-4410-be9c-065ec2a8f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tempfile\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, count, lit\n",
    "import folium  # Import für Kartenvisualisierung\n",
    "import time  # Import für Zeitmessung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2632a8-ae20-4a15-9e66-a0093c188de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globale Konfigurationsvariablen\n",
    "# Pfad für temporäre Speicherung\n",
    "temp_storage_path = \"./data/temp\"  # Konfigurierbarer Speicherpfad für temporäre Dateien\n",
    "os.makedirs(temp_storage_path, exist_ok=True)  # Ordner erstellen, falls er nicht existiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60415093-8a2a-443b-a60c-efa73c858684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der URLs\n",
    "csv_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\"]\n",
    "  #  \"https://web.ais.dk/aisdata/aisdk-2024-03-03.zip\",\n",
    "  #  \"https://web.ais.dk/aisdata/aisdk-2024-03-04.zip\",\n",
    "  # \"https://web.ais.dk/aisdata/aisdk-2024-03-05.zip\"\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc77e413-36d9-4d66-83fb-98656c3e9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Zeit für das Herunterladen und Entpacken der Dateien beträgt: 406.66 Sekunden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Schritt 1: Spark-Session erstellen\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schritt 2: Funktion zum Herunterladen, Entpacken und Speichern von CSV-Dateien\n",
    "def download_and_unzip_to_temp_csv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        csv_filename = z.namelist()[0]  # Der Name der CSV-Datei im ZIP-Archiv\n",
    "        with z.open(csv_filename) as csv_file:\n",
    "            temp_file_path = os.path.join(temp_storage_path, csv_filename)\n",
    "            with open(temp_file_path, \"wb\") as temp_file:\n",
    "                temp_file.write(csv_file.read())\n",
    "            return temp_file_path\n",
    "\n",
    "# Schritt 4: Paralleles Herunterladen und Speichern der CSV-Dateien in temporären Pfaden\n",
    "start_time = time.time()  # Startzeitpunkt\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    csv_file_paths = list(executor.map(download_and_unzip_to_temp_csv, csv_urls))\n",
    "end_time = time.time()  # Endzeitpunkt\n",
    "\n",
    "# Zeitmessung für den Download\n",
    "print(f\"Die Zeit für das Herunterladen und Entpacken der Dateien beträgt: {end_time - start_time:.2f} Sekunden\")\n",
    "\n",
    "\n",
    "# Schritt 5: CSV-Dateien mit Spark einlesen und kombinieren\n",
    "# Erstelle eine Liste von DataFrames für jede CSV-Datei\n",
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in csv_file_paths]\n",
    "\n",
    "# Kombiniere alle DataFrames zu einem großen DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca1ee5e0-3317-43d3-87e8-c559bfa8a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "|        # Timestamp|Type of mobile|     MMSI| Latitude|Longitude| Navigational status| ROT| SOG|  COG|Heading|    IMO|Callsign|Name|Ship type|Cargo type|Width|Length|Type of position fixing device|Draught|Destination| ETA|Data source type|   A|   B|   C|   D|\n",
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "|01/03/2024 00:00:00|       Class A|219000873| 56.99091|10.304543|Under way using e...|NULL| 0.0| 30.2|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|  Base Station|  2190068| 56.44726|10.945872|       Unknown value|NULL|NULL| NULL|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                           GPS|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219016683|56.800165| 9.024933|Under way using e...| 0.0| 0.0|257.3|     17|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219000615|56.967093| 9.224287|Restricted maneuv...| 0.0| 2.3| 69.8|     67|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|  Base Station|  2190071|57.110043| 8.648282|       Unknown value|NULL|NULL| NULL|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                           GPS|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219017664| 56.97495|  8.92253|Under way using e...| 0.0| 0.0|349.9|    201|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219002686|56.795143|  8.86396|Under way using e...| 0.0| 0.0|116.1|    213|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219030053|57.058252| 9.900817|       Unknown value|NULL| 0.0| 15.8|   NULL|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|219670000|55.463782| 8.444915|              Moored| 0.0| 0.0|321.0|    118|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "|01/03/2024 00:00:00|       Class A|211417590|54.524345|12.675237|Under way using e...| 0.0| 0.6|286.8|     23|Unknown| Unknown|NULL|Undefined|      NULL| NULL|  NULL|                     Undefined|   NULL|    Unknown|NULL|             AIS|NULL|NULL|NULL|NULL|\n",
      "+-------------------+--------------+---------+---------+---------+--------------------+----+----+-----+-------+-------+--------+----+---------+----------+-----+------+------------------------------+-------+-----------+----+----------------+----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:====================================================>   (42 + 3) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Originale Datensatz hat 31817670 Einträge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Schritt 6: Einige Beispielzeilen ausgeben, um mögliche MMSI-Nummern anzuzeigen\n",
    "combined_df.show(10)\n",
    "\n",
    "# Originale Anzahl der Einträge ausgeben\n",
    "print(f\"Der Originale Datensatz hat {combined_df.count()} Einträge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec710fc9-c2e8-441c-ae18-4fca9a94efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:====================================================>   (42 + 3) / 45]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der angepasste Datensatz hat 29508390 Einträge.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# 2. Basisstationen herausfiltern, da diese keine Navigationsdaten anzeigen (\"Type of mobile\" != \"Base Station\")\n",
    "########################################################\n",
    "\n",
    "# Prüfen, ob die Spalte \"Type of mobile\" existiert und filtern\n",
    "if \"Type of mobile\" in combined_df.columns:\n",
    "    combined_df = combined_df.filter(col(\"Type of mobile\") != \"Base Station\")\n",
    "else:\n",
    "    print(\"Warnung: 'Type of mobile' Spalte nicht vorhanden, Überspringe diesen Schritt.\")\n",
    "\n",
    "print(f\"Der angepasste Datensatz hat {combined_df.count()} Einträge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a4d614e-9200-4332-ad5f-fb8feacd2d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# 3. Nur relevante Spalten behalten, um Datenmenge zu reduzieren\n",
    "########################################################\n",
    "\n",
    "relevant_columns = [\"MMSI\", \"Latitude\", \"Longitude\", \"# Timestamp\"]\n",
    "combined_df = combined_df.select(*relevant_columns)\n",
    "\n",
    "########################################################\n",
    "# 4. Timestamp in datetime konvertieren\n",
    "########################################################\n",
    "\n",
    "combined_df = combined_df.withColumn(\"# Timestamp\", to_timestamp(col(\"# Timestamp\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "########################################################\n",
    "# 5. Filtern von MMSI-Nummern, die genug Datenpunkte haben, damit vernünftige Routen angezeigt werden\n",
    "########################################################\n",
    "\n",
    "# Anzahl Datenpunkte pro MMSI bestimmen\n",
    "mmsi_counts = combined_df.groupBy(\"MMSI\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Schwelle definieren (z.B. mindestens 50 Punkte)\n",
    "threshold = 50\n",
    "valid_mmsi = mmsi_counts.filter(col(\"count\") >= threshold).select(\"MMSI\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Gefilterter DataFrame nur mit MMSI, die genügend Datenpunkte haben\n",
    "filtered_by_count_df = combined_df.filter(col(\"MMSI\").isin(valid_mmsi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "065e73f9-4d8e-47ee-a11d-c908afb1d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Route wurde erfolgreich als 'ship_route.html' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# 6. Nach bestimmter MMSI und Zeitspanne filtern + Route plotten\n",
    "########################################################\n",
    "\n",
    "mmsi_number = 219016832  # Ersetze mit deiner MMSI\n",
    "\n",
    "# Definiere Start- und Endzeitpunkt (im Format \"dd/MM/yyyy HH:MM:SS\")\n",
    "start_str = \"01/03/2024 00:00:00\"  # Startzeitpunkt\n",
    "end_str = \"01/03/2024 06:59:59\"    # Endzeitpunkt\n",
    "\n",
    "# Konvertiere Start- und Endzeit in datetime-Objekte\n",
    "start_dt = to_timestamp(lit(start_str), \"dd/MM/yyyy HH:mm:ss\")\n",
    "end_dt = to_timestamp(lit(end_str), \"dd/MM/yyyy HH:mm:ss\")\n",
    "\n",
    "# Prüfen, ob MMSI genug Daten hat\n",
    "if mmsi_number not in valid_mmsi:\n",
    "    print(f\"MMSI {mmsi_number} hat nicht genügend Datenpunkte, um eine aussagekräftige Route anzuzeigen.\")\n",
    "else:\n",
    "    # Nach MMSI und Zeitspanne filtern\n",
    "    route_df = filtered_by_count_df.filter(\n",
    "        (col(\"MMSI\") == mmsi_number) &\n",
    "        (col(\"# Timestamp\") >= start_dt) &\n",
    "        (col(\"# Timestamp\") <= end_dt)\n",
    "    ).orderBy(\"# Timestamp\")\n",
    "\n",
    "    # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "    if route_df.count() == 0:\n",
    "        print(f\"Keine Daten für MMSI {mmsi_number} zwischen {start_str} und {end_str}\")\n",
    "    else:\n",
    "        # Daten in Pandas konvertieren, um Karte zu erstellen\n",
    "        pandas_df = route_df.toPandas()\n",
    "\n",
    "        # Karte erstellen und Route plotten\n",
    "        mean_lat = pandas_df[\"Latitude\"].mean()\n",
    "        mean_lon = pandas_df[\"Longitude\"].mean()\n",
    "        \n",
    "        route_map = folium.Map(location=[mean_lat, mean_lon], zoom_start=8)\n",
    "        \n",
    "        # Koordinatenliste für PolyLine\n",
    "        coords = pandas_df[[\"Latitude\", \"Longitude\"]].values.tolist()\n",
    "        \n",
    "        # PolyLine hinzufügen\n",
    "        folium.PolyLine(coords, color=\"blue\", weight=2.5, opacity=1).add_to(route_map)\n",
    "        \n",
    "        # Karte speichern\n",
    "        route_map.save(\"ship_route.html\")\n",
    "        print(\"Route wurde erfolgreich als 'ship_route.html' gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f31b2-e8de-4809-8142-eeade1487d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
