{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff7a69d9-359d-4e41-9797-9fde49108416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import folium\n",
    "import time  # Import fÃ¼r Zeitmessung\n",
    "import os\n",
    "\n",
    "# Directory for storing downloaded and extracted CSV files\n",
    "local_storage_path = \"./data/csv_files\"\n",
    "os.makedirs(local_storage_path, exist_ok=True)  # Create the directory if it does not exist\n",
    "\n",
    "# Spark-Session starten\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ShipRouteOptimization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Optional: Log-Level reduzieren fÃ¼r weniger Output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e103e9f7-def0-415b-9ebd-4db55e2dbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Function to download, extract, and save CSV from ZIP archive if not already downloaded\n",
    "def download_and_unzip_csv(url):\n",
    "    # Get the name of the ZIP file from the URL\n",
    "    zip_filename = url.split(\"/\")[-1]\n",
    "    csv_filename = zip_filename.replace(\".zip\", \".csv\")\n",
    "    csv_filepath = os.path.join(local_storage_path, csv_filename)\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(csv_filepath):\n",
    "        print(f\"File already exists: {csv_filepath}, skipping download.\")\n",
    "        return pd.read_csv(csv_filepath)  # Load the CSV into a DataFrame\n",
    "    \n",
    "    # Download and extract the ZIP file\n",
    "    print(f\"Downloading and extracting: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    zipfile_bytes = BytesIO(response.content)\n",
    "    with zipfile.ZipFile(zipfile_bytes, 'r') as z:\n",
    "        with z.open(z.namelist()[0]) as csv_file:\n",
    "            # Save the extracted CSV locally\n",
    "            with open(csv_filepath, \"wb\") as output_file:\n",
    "                output_file.write(csv_file.read())\n",
    "    \n",
    "    # Load the CSV into a DataFrame\n",
    "    return pd.read_csv(csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ddb90a-6abf-4ce9-a1ca-612f954b344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: List of CSV (ZIP) URLs\n",
    "csv_urls = [\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-01.zip\",\n",
    "    \"https://web.ais.dk/aisdata/aisdk-2024-03-02.zip\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5db2088-e6c9-4228-bb6e-0b388a9cc204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ./data/csv_files/aisdk-2024-03-01.csv, skipping download.File already exists: ./data/csv_files/aisdk-2024-03-02.csv, skipping download.\n",
      "\n",
      "The download time for 2 files with Pandas is 1 minutes and 34 seconds.\n",
      "The combined dataset contains 31817670 entries.\n",
      "The download time for Pandas was successfully updated in the README.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Parallel downloading, processing, and storing CSV files\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    dfs = list(executor.map(download_and_unzip_csv, csv_urls))  # Download or load files in parallel\n",
    "\n",
    "end_time = time.time()  # End time\n",
    "\n",
    "# Convert elapsed time to minutes and seconds\n",
    "elapsed_time = end_time - start_time\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "total_files = len(csv_urls)  # Count the number of files\n",
    "download_time = f\"The download time for {total_files} files with Pandas is {minutes} minutes and {seconds} seconds.\"\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the download time and dataset information\n",
    "print(download_time)\n",
    "print(f\"The combined dataset contains {len(combined_df)} entries.\")\n",
    "\n",
    "# Automatically update the README file\n",
    "try:\n",
    "    # Read the current contents of the README file\n",
    "    with open(\"README.md\", \"r\") as readme:\n",
    "        lines = readme.readlines()\n",
    "\n",
    "    # Update or add the section\n",
    "    updated_lines = []\n",
    "    section_found = False\n",
    "    for line in lines:\n",
    "        if line.strip() == \"### Download Time Results with Pandas\":\n",
    "            # Replace the next line with the updated time\n",
    "            updated_lines.append(line)\n",
    "            updated_lines.append(f\"{download_time}\\n\")\n",
    "            section_found = True\n",
    "        elif not section_found or line.strip() != download_time:\n",
    "            updated_lines.append(line)\n",
    "\n",
    "    # If the section was not found, append it at the end\n",
    "    if not section_found:\n",
    "        updated_lines.append(\"\\n### Download Time Results with Pandas\\n\")\n",
    "        updated_lines.append(f\"{download_time}\\n\")\n",
    "\n",
    "    # Write the updated content back to the README file\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.writelines(updated_lines)\n",
    "\n",
    "    print(\"The download time for Pandas was successfully updated in the README.\")\n",
    "except FileNotFoundError:\n",
    "    # If the README does not exist, create it\n",
    "    with open(\"README.md\", \"w\") as readme:\n",
    "        readme.write(\"### Download Time Results with Pandas\\n\")\n",
    "        readme.write(f\"{download_time}\\n\")\n",
    "    print(\"The README file was created and the download time was added.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to the README file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56ec1e60-6ef6-496b-b3bc-2b6e55d3c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           # Timestamp Type of mobile       MMSI   Latitude  Longitude  \\\n",
      "0  01/03/2024 00:00:00        Class A  219000873  56.990910  10.304543   \n",
      "1  01/03/2024 00:00:00   Base Station    2190068  56.447260  10.945872   \n",
      "2  01/03/2024 00:00:00        Class A  219016683  56.800165   9.024933   \n",
      "3  01/03/2024 00:00:00        Class A  219000615  56.967093   9.224287   \n",
      "4  01/03/2024 00:00:00   Base Station    2190071  57.110043   8.648282   \n",
      "5  01/03/2024 00:00:00        Class A  219017664  56.974950   8.922530   \n",
      "6  01/03/2024 00:00:00        Class A  219002686  56.795143   8.863960   \n",
      "7  01/03/2024 00:00:00        Class A  219030053  57.058252   9.900817   \n",
      "8  01/03/2024 00:00:00        Class A  219670000  55.463782   8.444915   \n",
      "9  01/03/2024 00:00:00        Class A  211417590  54.524345  12.675237   \n",
      "\n",
      "          Navigational status  ROT  SOG    COG  Heading  ... Length  \\\n",
      "0      Under way using engine  NaN  0.0   30.2      NaN  ...    NaN   \n",
      "1               Unknown value  NaN  NaN    NaN      NaN  ...    NaN   \n",
      "2      Under way using engine  0.0  0.0  257.3     17.0  ...    NaN   \n",
      "3  Restricted maneuverability  0.0  2.3   69.8     67.0  ...    NaN   \n",
      "4               Unknown value  NaN  NaN    NaN      NaN  ...    NaN   \n",
      "5      Under way using engine  0.0  0.0  349.9    201.0  ...    NaN   \n",
      "6      Under way using engine  0.0  0.0  116.1    213.0  ...    NaN   \n",
      "7               Unknown value  NaN  0.0   15.8      NaN  ...    NaN   \n",
      "8                      Moored  0.0  0.0  321.0    118.0  ...    NaN   \n",
      "9      Under way using engine  0.0  0.6  286.8     23.0  ...    NaN   \n",
      "\n",
      "  Type of position fixing device Draught Destination  ETA  Data source type  \\\n",
      "0                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "1                            GPS     NaN     Unknown  NaN               AIS   \n",
      "2                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "3                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "4                            GPS     NaN     Unknown  NaN               AIS   \n",
      "5                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "6                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "7                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "8                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "9                      Undefined     NaN     Unknown  NaN               AIS   \n",
      "\n",
      "    A   B   C   D  \n",
      "0 NaN NaN NaN NaN  \n",
      "1 NaN NaN NaN NaN  \n",
      "2 NaN NaN NaN NaN  \n",
      "3 NaN NaN NaN NaN  \n",
      "4 NaN NaN NaN NaN  \n",
      "5 NaN NaN NaN NaN  \n",
      "6 NaN NaN NaN NaN  \n",
      "7 NaN NaN NaN NaN  \n",
      "8 NaN NaN NaN NaN  \n",
      "9 NaN NaN NaN NaN  \n",
      "\n",
      "[10 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Combine all DataFrames into a single large DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display a few sample rows to show possible MMSI numbers\n",
    "print(combined_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3322962f-d771-487d-b9ba-a8f5085978c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset contains 31817670 entries.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The original dataset contains {len(combined_df)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88069291-d73c-4517-8373-408699838e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 2. Filter out base stations (\"Type of mobile\" != \"Base Station\")\n",
    "########################################################\n",
    "\n",
    "# Keep only rows that are not \"Base Station\"\n",
    "if \"Type of mobile\" in combined_df.columns:\n",
    "    combined_df = combined_df[combined_df[\"Type of mobile\"] != \"Base Station\"]\n",
    "else:\n",
    "    print(\"Warning: 'Type of mobile' column not found, skipping this step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6828bf94-75db-47f0-abbb-dd7b77ba4608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjusted dataset contains 29508390 entries.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The adjusted dataset contains {len(combined_df)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7d6e16-3e6c-4465-9d43-7ae5155e2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 3. Keep only relevant columns to reduce data size\n",
    "########################################################\n",
    "\n",
    "relevant_columns = [\"MMSI\", \"Latitude\", \"Longitude\", \"# Timestamp\"]\n",
    "combined_df = combined_df[relevant_columns]\n",
    "\n",
    "########################################################\n",
    "# 4. Convert Timestamp to datetime format\n",
    "########################################################\n",
    "\n",
    "combined_df['# Timestamp'] = pd.to_datetime(combined_df['# Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "\n",
    "########################################################\n",
    "# 5. Filter MMSI numbers with enough data points\n",
    "########################################################\n",
    "\n",
    "# Determine the number of data points per MMSI\n",
    "mmsi_counts = combined_df.groupby(\"MMSI\").size().reset_index(name=\"count\")\n",
    "\n",
    "# Define a threshold (e.g., at least 50 points)\n",
    "threshold = 50\n",
    "valid_mmsi = mmsi_counts[mmsi_counts[\"count\"] >= threshold][\"MMSI\"].unique()\n",
    "\n",
    "# Filtered DataFrame containing only MMSI numbers with enough data points\n",
    "filtered_by_count_df = combined_df[combined_df[\"MMSI\"].isin(valid_mmsi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9b05c5-61ae-4873-8831-133e339f9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 6. Filter by specific MMSI and time range + plot the route\n",
    "########################################################\n",
    "\n",
    "mmsi_number = 219016832  # Replace with your desired MMSI\n",
    "\n",
    "# Define start and end time (in the format \"dd/mm/yyyy HH:MM:SS\")\n",
    "start_str = \"01/03/2024 00:00:00\"  # March 3, 2024, 00:00\n",
    "end_str = \"01/03/2024 06:59:59\"    # March 3, 2024, 06:59\n",
    "\n",
    "# Convert start and end times to datetime objects\n",
    "start_dt = pd.to_datetime(start_str, format=\"%d/%m/%Y %H:%M:%S\", errors=\"coerce\")\n",
    "end_dt = pd.to_datetime(end_str, format=\"%d/%m/%Y %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "# Check if the MMSI has enough data points\n",
    "if mmsi_number not in valid_mmsi:\n",
    "    print(f\"MMSI {mmsi_number} does not have enough data points to display a meaningful route.\")\n",
    "else:\n",
    "    # Filter by MMSI and time range\n",
    "    route_df = filtered_by_count_df[\n",
    "        (filtered_by_count_df[\"MMSI\"] == mmsi_number) &\n",
    "        (filtered_by_count_df[\"# Timestamp\"] >= start_dt) &\n",
    "        (filtered_by_count_df[\"# Timestamp\"] <= end_dt)\n",
    "    ].sort_values(\"# Timestamp\")\n",
    "\n",
    "    # Check if filtered data is available\n",
    "    if route_df.empty:\n",
    "        print(f\"No data for MMSI {mmsi_number} between {start_dt} and {end_dt}\")\n",
    "    else:\n",
    "        # Create a map and plot the route\n",
    "        mean_lat = route_df[\"Latitude\"].mean()\n",
    "        mean_lon = route_df[\"Longitude\"].mean()\n",
    "        \n",
    "        route_map = folium.Map(location=[mean_lat, mean_lon], zoom_start=8)\n",
    "        \n",
    "        # List of coordinates for PolyLine\n",
    "        coords = route_df[[\"Latitude\", \"Longitude\"]].values.tolist()\n",
    "        \n",
    "        # Add the PolyLine\n",
    "        folium.PolyLine(coords, color=\"blue\", weight=2.5, opacity=1).add_to(route_map)\n",
    "        \n",
    "        # Optional: Mark points (commented out if not needed)\n",
    "        # for _, row in route_df.iterrows():\n",
    "        #     folium.CircleMarker(\n",
    "        #         location=[row['Latitude'], row['Longitude']],\n",
    "        #         radius=2,\n",
    "        #         color='red',\n",
    "        #         fill=True,\n",
    "        #         fill_color='red',\n",
    "        #         fill_opacity=0.7,\n",
    "        #         popup=f\"Timestamp: {row['# Timestamp']}\"\n",
    "        #     ).add_to(route_map)\n",
    "        \n",
    "        # Save or display the map\n",
    "        route_map.save(\"ship_route.html\")\n",
    "        route_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c4d23-af37-447f-b83a-1abf01b5a190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
