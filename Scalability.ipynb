{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, count, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_storage_path = \"./data/csv_files\"\n",
    "os.makedirs(local_storage_path, exist_ok=True)  # Create the directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 01:51:10 WARN Utils: Your hostname, MBAO.local resolves to a loopback address: 127.0.0.1; using 192.168.0.117 instead (on interface en0)\n",
      "25/02/01 01:51:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 01:51:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsTest1 = [\n",
    "    \"./data/csv_files/aisdk-2024-03-01.csv\"\n",
    "] #unfertig muss hier noch pfade auf unsere neuen Ã¤ndern\n",
    "\n",
    "pathsTest2 = [\n",
    "    \"./data/csv_files/aisdk-2024-03-01.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-02.csv\"\n",
    "]\n",
    "\n",
    "pathsTest3 = [\n",
    "    \"./data/csv_files/aisdk-2024-03-01.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-02.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-03.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-04.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-05.csv\"\n",
    "]\n",
    "\n",
    "pathsTest4 = [\n",
    "    \"./data/csv_files/aisdk-2024-03-01.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-02.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-03.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-04.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-05.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-01.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-02.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-03.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-04.csv\",\n",
    "    \"./data/csv_files/aisdk-2024-03-05.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests General Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 - One File - 2.63 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/01 01:51:26 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15512927"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest1]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 46<br>\n",
    "Input - 5.3 GiB<br>\n",
    "Time - 9.4 s<br>\n",
    "Note: The parameter \"InferSchema\" results in additional Jobs due to passing over the file and identifiying the data structure. This leads to a slightly higher amount of jobs than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 - 2 Files - 5.4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31817670"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest2]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 93<br>\n",
    "Input - 10.8 GiB<br>\n",
    "Time - 13.4 s <br>\n",
    "Note: The input is larger than the individual files used, this is because the size in the spark ui is not the filesize, but the size of the loaded objects. These objects needs more memory due to overhead in the spark data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3 - 5 Files - 13.32 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 226<br>\n",
    "Input - 26.5 GiB<br>\n",
    "Time - 32.9 s <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4 - 10 Files - 26.46 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156044574"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest4]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 451<br>\n",
    "Input - 52.9 GiB<br>\n",
    "Time - 1 m 7 s <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion General Scalability\n",
    "\n",
    "In these tests the scalability of spark was demonstrated using the data of our project and a simplified version of our projects code. In every test the amount of tasks created and executed by spark, the size of the input data and the execution time were noted. The tests were conducted using sparks local mode. If you compare the results of the individual tests you see that as the amount of data increases linearly, so does the input size, the amount of spark tasks and the execution time. This behaviour is one of the things making spark a good tool in Big Data use cases since a linear increase in performence is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Hardware Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Hardware Analysis\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
