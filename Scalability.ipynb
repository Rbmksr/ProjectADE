{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to answer general questions about Sparkâ€™s scalability and its comparison with Pandas. It is structured into three sections:<br>\n",
    "\n",
    "1. General Scalability of Spark (Tests 1-4)\n",
    "    These tests are designed to demonstrate Spark's behavior with varying data volumes.\n",
    "2. Hardware Scalability of Spark (Tests 5-10)\n",
    "    These tests demonstrate the impact of different hardware configurations on Spark's performance.\n",
    "3. Direct Comparison with Pandas (Tests 11-12)\n",
    "    These tests directly compare Pandas and Spark across different data volumes, analyzing their impact on execution time and hardware usage.\n",
    "\n",
    "To simplify the problem, the code executed here represents only a portion of our overall use case. In all tests, data files are read and then combined into a single DataFrame. To verify correctness, the number of rows in the DataFrame is printed after each test. Any discrepancies in row count may indicate errors in data processing.<br>\n",
    "\n",
    "Note: To properly test Spark setups with different configurations, you need to terminate the old Spark sessions and then execute the cells containing the Spark session builders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, count, lit\n",
    "import pandas as pd\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_storage_path = \"./data/csv_files\"\n",
    "os.makedirs(local_storage_path, exist_ok=True)  # Create the directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:52:40 WARN Utils: Your hostname, MBAO.local resolves to a loopback address: 127.0.0.1; using 192.168.0.113 instead (on interface en0)\n",
      "25/02/04 00:52:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/04 00:52:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/04 00:52:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: If you encounter any errors related to file paths, ensure that the placement of \"/\" symbols is correct.\n",
    "\n",
    "pathsTest1 = [\n",
    "    local_storage_path + \"/aisdk-2024-03-01.csv\"\n",
    "] \n",
    "\n",
    "pathsTest2 = [\n",
    "    local_storage_path + \"/aisdk-2024-03-01.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-02.csv\"\n",
    "]\n",
    "\n",
    "pathsTest3 = [\n",
    "    local_storage_path + \"/aisdk-2024-03-01.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-02.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-03.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-04.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-05.csv\"\n",
    "]\n",
    "\n",
    "pathsTest4 = [\n",
    "    local_storage_path + \"/aisdk-2024-03-01.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-02.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-03.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-04.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-05.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-01.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-02.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-03.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-04.csv\",\n",
    "    local_storage_path + \"/aisdk-2024-03-05.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests General Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 - One File - 2.63 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15512927"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest1]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 46<br>\n",
    "Input - 5.3 GiB<br>\n",
    "Time - 9.4 s<br>\n",
    "Note: The parameter \"InferSchema\" results in additional Jobs due to passing over the file and identifiying the data structure. This leads to a slightly higher amount of jobs than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 - 2 Files - 5.4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:52:52 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31817670"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest2]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 93<br>\n",
    "Input - 10.8 GiB<br>\n",
    "Time - 13.4 s <br>\n",
    "Note: The input is larger than the individual files used, this is because the size in the spark ui is not the filesize, but the size of the loaded objects. These objects needs more memory due to overhead in the spark data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3 - 5 Files - 13.32 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 226<br>\n",
    "Input - 26.5 GiB<br>\n",
    "Time - 32.9 s <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4 - 10 Files - 26.46 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156044574"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest4]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Tasks - 451<br>\n",
    "Input - 52.9 GiB<br>\n",
    "Time - 1 m 7 s <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion General Scalability\n",
    "\n",
    "In these tests the scalability of spark was demonstrated using the data of our project and a simplified version of our projects code. In every test the amount of tasks created and executed by spark, the size of the input data and the execution time were noted. The tests were conducted using sparks local mode. If you compare the results of the individual tests you see that as the amount of data increases linearly, so does the input size, the amount of spark tasks and the execution time. This behaviour is one of the things making spark a good tool in Big Data use cases since a linear increase in performence is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Hardware Scalability - CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5 - 2 Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:54:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Hardware Analysis\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 1m 30.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6 - 4 Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:55:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Hardware Analysis\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 49.6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7 - 8 Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:56:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Hardware Analysis\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 36.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Hardware Scalability - Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 8 - 4GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:56:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Hardware Analysis\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.executor.memory\",\"4g\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 52.5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 9 - 8GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:57:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Hardware Analysis\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 47.5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 10 - 16GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:57:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Hardware Analysis\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.executor.memory\",\"16g\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "Time - 50s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Hardware Scalability\n",
    "All previous tests executed the simplified Spark code with the same amount of data (5 files) but different hardware configurations. The first few tests (5-7) all ran with 4 GB of RAM but with different numbers of cores (2, 4, 8). These tests show that increasing the number of cores significantly improves the application's performance. This is most likely because increasing the number of cores also increases the potential for parallelization.<br>\n",
    "The next few tests (7-10) all ran with 4 cores but with different amounts of memory (4 GB, 8 GB, 16 GB). In these tests, the increase in memory had an extremely small impact on the application's performance. This is most likely because the test application is relatively simple, and 4 GB was already sufficient for the required tasks. This suggests that an individual node generally does not need large amounts of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Hardware Comparison Pandas - Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 11 - Comparison with 2 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 54.5%\n",
      "Memory Usage: 78.9%\n",
      "31817670\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataframes = [pd.read_csv(path) for path in pathsTest2]\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "# Get the number of rows  \n",
    "row_count = len(combined_df)\n",
    "print(row_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 48.8s<br>\n",
    "Memory Usage Read - 43.9%<br>\n",
    "Memory Usage Union - 64.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 00:59:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Comparison Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 60.1%\n",
      "Memory Usage: 43.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31817670"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest2]\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 10.2s<br>\n",
    "Memory Usage Read - 30.5%<br>\n",
    "Memory Usage Union - 30.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 12 - Comparison with 5 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 55.7%\n",
      "Memory Usage: 82.8%\n",
      "78022287\n"
     ]
    }
   ],
   "source": [
    "dataframes = [pd.read_csv(path) for path in pathsTest3]\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "# Get the number of rows  \n",
    "row_count = len(combined_df)\n",
    "print(row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 2m 2.6s - 122.6s<br>\n",
    "Memory Usage Read - 54.7%<br>\n",
    "Memory Usage Union - 82.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Comparison Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 52.7%\n",
      "Memory Usage: 28.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78022287"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "Time - 10.2s<br>\n",
    "Memory Usage Read - 54.3%<br>\n",
    "Memory Usage Union - 31.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Comparison Pandas - Spark\n",
    "\n",
    "In both tests, the Spark implementation was significantly faster than the Pandas version (Test 1: 38.6s; Test 2: 77.7s). The execution time nearly doubled, aligning with the increase in data volume. Notably, Spark's processing time for the test with five files was still shorter than Pandas' time for just two files. Additionally, in both tests, Pandas exhibited consistently higher memory usage while reading and merging the data frames. However, during the tests, memory usage varied significantly. These results demonstrate that, already for data volumes similar to those used in this test, Spark is generally the superior (i.e., faster and more memory-efficient) solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
