{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, count, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIS Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsTest1 = [] #unfertig muss hier noch pfade auf unsere neuen Ã¤ndern\n",
    "\n",
    "pathsTest2 = []\n",
    "\n",
    "pathsTest3 = []\n",
    "\n",
    "pathsTest4 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1 - One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest1]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Tasks - 46 Input - 5.3 GiB InferSchema results in additional Job due to passing over file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2 - 2 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest2]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Tasks - 93 Input - 10.8 GiB -> Note that the input is larger than the used files, this is because the size in the spark ui is not the filesize, but the size of the loaded object. This object needs more memory due to overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3 - 5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest3]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Tasks - 226 Input - 26.5 GiB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [spark.read.csv(path, header=True, inferSchema=True) for path in pathsTest4]\n",
    "\n",
    "# Combine all DataFrames into a single large DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: Tasks - 451 Input - 52.9 GiB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
